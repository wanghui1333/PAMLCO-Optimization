clear all;
clc;
x_train=[0.053	13.554	2.739	3.29	114.399	2.708	4.939	136.89	-19.487	58.051	32.694	34.558	0.66	108.29
0.058	14.636	2.652	3.169	116.906	2.703	4.225	138.63	-19.531	57.156	43.193	41.456	0.657	120.232
0.045	1.165	0.744	0.682	17.473	2.577	5.037	26.583	-49.524	53.131	99.634	139.928	6.071	24.38
0.031	1.583	1.064	0.641	18.585	2.624	5.42	23.872	-51.28	51.07	164.503	230.812	140.461	20.142
0.018	2.161	0.587	0.772	23.531	2.649	5.432	25.811	-53.537	52.473	165.818	209.354	84.416	21.879
0.024	2.283	0.434	0.438	18.059	2.655	5.391	30.172	-54.57	54.27	132.43	149.481	133.438	22.334
0.073	6.414	1.533	1.918	63.095	2.679	6.509	78.627	-53.46	61.147	17.274	13.83	10.364	67.691
0.088	6.717	1.492	2.225	69.981	2.598	6.394	73.352	-53.536	63.247	17.225	10.066	1.559	64.803
0.07	2.605	1.027	0.767	25.42	2.638	7.119	61.864	-57.887	58.763	37.26	32.46	31.301	53.306
0.019	2.761	1.533	0.884	28.213	2.701	6.977	37.832	-61.059	52.656	173.325	129.454	200.346	30.577
0.018	2.029	1.02	0.543	18.82	2.674	7.453	22.662	-69.158	53.264	535.967	407.985	315.54	18.187
0.014	1.928	1.214	0.647	20.245	2.68	6.859	20.906	-73.017	52.475	425.614	307.599	181.235	20.633
0.011	2.492	0.637	0.526	20.56	2.719	7.623	26.672	-74.351	50.959	385.875	236.051	204.875	22.27
0.011	1.759	0.956	0.557	17.863	2.763	8.333	25.711	-76.336	50.263	175.279	143.053	99.875	22.338
0.023	1.022	0.911	0.669	16.596	2.662	6.774	22.615	-68.564	53.725	342.531	271.821	80.964	15.529
0.019	1.394	0.312	0.536	15.856	2.686	6.414	22.734	-69.813	52.179	448.987	379.474	107.719	18.702
0.015	1.078	0.597	0.355	11.185	2.894	9.157	24.947	-71.85	52.964	563.635	545.035	819.36	22.789
0.017	1.815	2.737	0.564	18.233	2.721	7.75	46.847	-73.916	53.469	117.378	96.898	65.971	42.274
0.019	3.22	3.941	1.512	41.568	2.7	6.655	47.93	-67.076	53.226	80.239	65.004	22.212	43.784
0.01	2.024	2.345	0.73	22.166	2.711	6.71	36.167	-67.565	50.342	269.602	182.016	154.047	34.531
0.022	2.433	0.91	1.434	36.665	2.7	6.63	46.17	-69.669	53.342	162.337	128.91	169.339	44.419
0.024	2.33	2.143	0.895	26.5	2.704	7.194	68.725	-67.993	53.401	87.101	63.632	58.831	54.535
0.017	3.092	2.632	1.307	37.312	2.699	7.132	50.428	-70.542	51.865	117.69	84.763	45.865	46.005
0.028	4.296	0.807	1.31	42.727	2.716	6.105	61.435	-67.052	53.521	81.427	50.944	57.273	57.04
0.02	3.168	1.337	1.109	34.085	2.702	6.771	54.393	-73.515	52.324	118.571	86.364	63.015	44.522
0.02	3.168	1.337	1.109	34.085	2.702	6.771	54.393	-73.515	52.324	118.571	86.364	63.015	44.522
0.016	2.905	1.857	1.309	36.513	2.703	6.529	41.627	-76.777	51.502	211.739	147.167	128.065	37.964
0.027	2.104	0.704	0.748	22.836	2.705	6.374	60.965	-74.577	53.193	139.526	104.065	89.028	50.091
0.027	2.104	0.704	0.748	22.836	2.705	6.374	60.965	-74.577	53.193	139.526	104.065	89.028	50.091
0.011	4.767	0.782	1.049	40.114	2.688	6.806	32.495	-71.845	49.578	553.684	451.922	371.42	24.895
0.019	3.746	7.399	1.355	41.084	2.641	7	50.648	-71.252	53.491	15.288	16.972	6.387	51.323
0.013	2.266	1.567	0.952	27.245	2.709	7.025	23.109	-71.276	48.265	548.666	494.541	288.574	17.282
0.012	2.204	1.535	0.528	19.329	2.72	6.982	22.128	-70.692	48.59	956.505	691.003	479.593	21.391
0.013	3.198	2.368	0.689	26.637	2.711	6.639	33.93	-62.64	49.464	569.295	431.854	218.983	29.251
0.013	2.726	0.909	0.908	28.489	2.717	6.678	23.946	-60.755	49.306	958.384	661.984	543.984	21.627
0.015	1.803	0.896	0.535	17.657	2.709	7.004	28.096	-62.307	49.507	530.865	373.06	283.341	26.914
0.015	2.206	1.296	0.768	23.66	2.714	6.242	31.997	-58.847	49.507	731.403	513.1	462.831	26.82
0.01	3.211	1.81	0.935	31.139	2.689	7.069	25.364	-69.174	49.053	992.04	701.439	466.952	23.262
0.016	2.806	0.98	0.789	26.706	2.683	6.338	63.569	-70.527	51.852	167.724	161.18	99.975	50.622
0.008	1.504	0.86	0.654	18.47	2.692	6.598	22.079	-63.479	48.989	1767.305	1538.829	895.718	21.561
0.012	2.688	1.749	1.059	31.049	2.716	6.196	29.973	-51.153	49.508	1507.264	1218.065	545.768	27.395
0.013	0.908	0.892	0.79	18.271	2.708	6.449	27.272	-50.799	49.49	1615.49	1581.595	651.156	22.511
0.016	2.368	1.181	0.667	22.563	2.712	6.662	36.218	-52.554	49.755	893.913	669.628	361.389	30.629
0.015	1.857	1.866	0.868	23.909	2.698	6.366	27.488	-50.986	49.629	677.905	568.313	284.376	26.513
0.018	1.851	1.278	0.826	23.113	2.708	6.894	35.408	-52.223	50.581	425.922	435.564	376.549	29.552
0.012	1.62	1.189	0.759	20.892	2.706	7.07	29.655	-52.295	50.61	313.044	304.074	95.8	31.094
0.014	1.621	1.146	0.64	18.742	2.724	6.574	30.827	-51.293	49.677	1162.186	989.895	444.265	23.235
0.012	1.62	1.189	0.759	20.892	2.706	7.07	29.655	-52.295	50.61	313.044	304.074	95.8	31.094
0.013	2.47	1.595	0.797	25.358	2.697	7.467	32.193	-52.18	49.632	443.705	434.87	363.728	25.109
0.021	3.325	1.388	1.018	33.147	2.704	7.043	42.644	-49.403	53.755	229.082	232.308	146.599	44.278
0.019	2.26	1.28	0.995	27.993	2.702	7.034	41.345	-49.389	52.017	336.222	329.642	733.309	35.853
0.017	2.009	0.874	0.744	22.346	2.691	6.827	30.801	-51.818	50.076	628.388	622.788	417.675	29.829
0.022	2.904	1.408	0.703	25.588	2.707	7.081	40.811	-52.291	50.503	354.546	361.575	411.342	32.155
0.015	2.733	1.43	0.853	27.538	2.702	7.132	37.696	-52.654	49.583	560.354	536.714	455.346	31.168
];

y_train=[0.949367
0.265823
0.227848
0.303797
0.379747
0.797468
0.721519
0.227848
1.10127
1.10127
0.797468
0.265823
0.341772
0.379747
0.227848
0.227848
0.227848
0.227848
0.417722
0.303797
0.417722
0.493671
0.265823
0.227848
0.265823
0.455696
1.17722
0.759494
1.13924
0.759494
0.227848
0.227848
0.303797
0.56962
0.531646
1.63291
0.341772
0.265823
0.227848
0.227848
0.265823
0.227848
0.911392
1.55696
2.24051
1.51899
0.759494
0.455696
0.265823
0.265823
0.265823
0.227848
0.265823
0.227848
];



%对原始数据处理
% scale = [0.01, 0.0001, 0.01, 0.0001, 0.01, 0.1,1, 0.01];
% for idx=1:length(scale)
%     x_train(:, idx) = x_train(:, idx) * scale(idx);    
% end
%x_train=zscore(x_train);
% x_train=mapminmax(x_train,0,1);

% meanX = mean(x_train);
% stdX = std(x_train);
% x_train = bsxfun(@rdivide,bsxfun(@minus,x_train,meanX),stdX); 

kernel = [{'exponential'},{'squaredexponential'},{'matern32'},{'matern52'},{'rationalquadratic'},{'ardexponential'},{'ardsquaredexponential'},{'ardmatern32'},{'ardmatern52'},{'ardrationalquadratic'}];
N_kernels = length(kernel);
factor=[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0];
for idx=1:N_kernels-1
    for jdx=idx+1:N_kernels
        gprMdl_idx = fitrgp(x_train, y_train,'KernelFunction',char(kernel(idx)) ,'FitMethod','sr','PredictMethod','fic', 'Standardize',1);

        rng('default')
       
cvgprMdl_idx = crossval(gprMdl_idx,'KFold',50);
L1 = kfoldLoss(cvgprMdl_idx,'mode','individual');
L2 = kfoldLoss(cvgprMdl_idx);
mse = mean(L1);
ypred_idx(:,1) = kfoldPredict(cvgprMdl_idx);
        gprMdl_jdx = fitrgp(x_train, y_train,'KernelFunction',char(kernel(jdx)) ,'FitMethod','sr','PredictMethod','fic', 'Standardize',1);
         rng('default')
        
cvgprMdl_jdx = crossval(gprMdl_jdx,'KFold',50);
L3 = kfoldLoss(cvgprMdl_jdx,'mode','individual');
L4 = kfoldLoss(cvgprMdl_jdx);
mse = mean(L3);
ypred_jdx(:,1) = kfoldPredict(cvgprMdl_jdx);
        for kdx=1:length(factor)
            output_train = ypred_idx * factor(kdx) + ypred_jdx * (1-factor(kdx));
            
            if mean(abs(y_train-output_train)./y_train) <0.9
                save(strcat('results\', 'xiaohui', num2str(factor(kdx)), char(kernel(idx)), char(kernel(jdx))), 'factor');
            end
        end
    end
end




% gprMdl = fitrgp(x_train, y_train,'KernelFunction','ardrationalquadratic','FitMethod','sr','PredictMethod','fic', 'Standardize',1);
% rng('default')
% cvgprMdl = crossval(gprMdl,'KFold',60);
% L = kfoldLoss(cvgprMdl,'mode','individual')
% L2 = kfoldLoss(cvgprMdl)
% mse = mean(L)
% ypred = kfoldPredict(cvgprMdl);
% plot(y_train,'r.');
% hold on;
% plot(ypred,'b--.');
% axis([0 62 0 10]);
% legend('True response','GPR prediction','Location','Best');
% hold off;


